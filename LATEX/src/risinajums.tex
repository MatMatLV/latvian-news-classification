\chapter{Rāpuļa un klasifikācijas modeļu izveide}
\section{Datu izgūšana no ziņu portāliem ar rāpuli}
Lai veiktu izpēti, sakumā ir nepieciešams ievākt treniņdatus / valodas korpusu, kas raksturo problēmvidi – ziņu portālu rakstus. Praktiskai rāpuļa implementācijai tika izvēlēts Python ietvars “Scrapy”, ar kura palīdzību iespējams izveidot tīmekļa rāpuļus, kas pārmeklē mājaslapas un izvelk no tām datus strukturētā formā. Šis ietvars izvēlēts, jo tas ir viens no populārākajiem rīkiem šajā kategorijā un tas labi spēj apstrādāt un formatēt lielu datu apjomu. Tā kā tīmekļa rāpuļi ir jāpielāgo konkrētai mājaslapas struktūrai, lai iegūtu vēlamos datus, tika izvēlēts konkrēts portāls - delfi.lv, dēļ tā daudzveidīgā kategoriju klāsta un rakstu daudzuma. Lai palielinātu iegūstamo rakstu daudzveidību tika apsvērts pielietot rāpuli arī uz citiem ziņu portāliem, tomēr tiem visiem ir liels pārklājums savā starpā, pārpublicējot rakstus no ziņu aģentūrām kā LETA. Šāda pieeja sekojoši varētu novest pie dublikātiem datu kopā, radot iespēju vienādiem rakstiem parādīties gan apmācības, gan validācijas kopās vienlaicīgi. Darba ietvaros izveidots rāpulis, kas ievāc datus no delfi.lv portāla un saglabā tos JSON formā ar 4 pamatlaukiem – virsraksts, kategorija, saturs, hipersaite. Lai sašaurinātu problēmvidi un ierobežotu nepieciešamos resursus tika izvēlētas 10 apskatāmās kategorijas – mūzika, atpūta, kriminālziņas, finanses, tehnoloģijas, kino, literatūra, politika, sports, auto. Darbības princips rāpulim ir sekojošs:
\begin{itemize}
\item Rāpulim sākotnēji apskatāmās saites noradām kā 10 izvēlēto kategoriju lapas (piemēram, politikas ziņām - https://www.delfi.lv/193/politics?page=1).
\item No lapām iegūstam hipersaites uz individuāliem rakstiem, ierobežojot tālāk apskatāmās saites tā, lai tās aizvien piederētu apskatāmajām kategorijām un nesaturētu nevēlamas saites (kā komentāru lapas rakstiem).
\item Apstrādājam katra raksta lapu, piefiksējot hipersaiti un dažādas komponentes - virsrakstu, kategoriju, saturu, atlasot tos pēc HTML elementu atbilstības konkrētas komponentes kritērijiem.
\item Saglabājam katru rakstu JSON formātā un ierakstam failā, atkārtojot procedūru līdz vairs neatrodam unikālus rakstus ko rāpulim apmeklēt.
\end{itemize}

Scrapy ietvars kopumā dod iespēju diezgan efektīvi realizēt šādu rāpuli – sākot no apskatāmo rakstu ierobežošanas līdz elementu atlasei un apstrādei. Salīdzinoši grūtāk ir tieši formalizēt kādus selektorus izvēlēties elementu atlasei, jo lapu formatējums starp dažādām kategorijām mēdz būt atšķirīgs un pat vienas kategorijas ietvaros tika novērots ka senākiem rakstiem formatējums var atšķirties no jaunāko rakstu formatējuma. Papildus tam arī ne visi raksti ir derīgi datu ieguvei, piemēram sastopami raksti kas satur tikai foto un video galerijas ar minimāliem aprakstiem, sastopami arī maksas raksti ar tikai vienu publiski pieejamu rindkopu.

Izgūta raksta piemēru JSON formātā iespējams apskatīt \ref{appendix:raksta_piemers} pielikumā. Rezultātā tika ievākti 13762 raksti ar sadalījumu pa kategorijām kāds redzams tabulā \ref{tab:rakstu_kategorijas}
\begin{table}[H]
\centering
\caption{\label{tab:rakstu_kategorijas}}
\textbf{Ievākto rakstu sadalījums pa kategorijām\\}
\begin{tblr}{
  hlines,
  vlines,
}
Kategorija    & Raksti  \\
Mūzika & 1722  \\
Atpūta & 1523  \\
Kriminālziņas & 1517  \\
Finanses & 1363  \\
Tehnoloģijas & 1333  \\
Kino & 1282  \\
Literatūra & 1277  \\
Politika & 1263  \\
Sports & 1250  \\
Auto & 1232
\end{tblr}
\end{table}

Ar rāpuli izgūtie teksti papildus tika manuāli pārbaudīti un attīrīti no nevēlamiem datiem – iegulti koda fragmenti audio /video atskaņotājiem, hipersaites. Ievācot rakstus novērots, ka bez rakstu satura atšķiras arī vidējie rakstu garumi katrā kategorijā. Piemēram atpūtas ziņām raksturīgi gari raksti ar vidēji vairāk nekā 662 vārdiem, savukārt auto, sporta un kriminālziņām – krietni īsāki raksti (īpaši auto ziņām ar vidējo rakstu garumu ap 227 vārdiem). Šāda atšķirība garumos varētu atstāt ietekmi uz konkrētu kategoriju klasifikāciju akurātumu. Rakstu iedalījumu garumos sīkāk iespējams apskatīt \ref{appendix:kategorijas_wc} pielikumā.

\section{Klasisko mašīnmācīšanās algortimu implementācija}

\subsubsection{Izmantotie rīki}
Viena no Python valodas populārākajām mašīnmācīšanās bibliotēkām, kas palīdz risināt problēmas kā klasteru veidošana, regresija, klasifikācija, dimensiju skaita samazināšana, ir ‘sckit-learn’. Autors ir izvēlējies lietot šo bibliotēku lai atvieglotu plaši lietotu klasifikācijas algoritmu implementāciju (Naivā Bejesa metode, loģistiskā regresija, lēmumu koki,  atbalsta vektoru mašīnas). 

Svarīga loma valodas apstrādē arī ir ievades tokenizācijai, tās veikšanai iespējams izmantot Python bibliotēkas kā NLTK un spaCy. Tieši šīs divas bibliotēkas ir populārākās un spēj tikt galā ar biežām tokenizācijas problēmām kā saīsinājumu, pieturzīmju un simbolu atdalīšana. Darba ietvaros tokenizācijas nolūkiem tika izvēlēts pielietot spaCy.

\subsection{Priekšapstrāde un vektorizācija}
\subsubsection{Tokenizācija un tekstu attīrīšana}
Teksta apstrāde tiek sākta ar teksta tokenizāciju, izmantojot spaCy bibliotēkā iebūvētās metodes. Kad teksts ir sadalīts vārdos – apstrāde tiek turpināta ar visu vārdu pārvēršanu formā ar visiem mazajiem burtiem, tiek izņemtas pieturzīmes un stopvārdi.

Konkrētāk apskatot stopvārdu atmešanu - dažādās Python bibliotēkās ir iekļauti saraksti ar stopvārdiem izplatītām valodām, tomēr latviešu valodai šāds sarakts jādefinē neatkarīgi. Tika veikta izpēte par to vai šāds saraksts jau ir publiski pieejams un kā viens no populārākajiem atrasts ‘stopwords-lv’ repozitorijs iekš github. Lai gan tas ir izmantojams kā labs pamats un uzskaita palīgvārdus (saikļus, prievārdus, partikulas), trūkst citas svarīgas morfoloģiskās grupas kā vietniekvārdi (attieksmes vietniekvārdi – kurš, kura u.c., norādāmie vietniekvārdi – šis, šī, tas, tā, viņš u.c, kā arī locījumi šiem vārdiem), jo arī šo vārdu esamība neraksturo teksta fragmenta jēgu vai piederību kādai kategorijai. Darba ietvaros izveidots uzlabots stopvārdu saraksts latviešu valodai, kas labāk spētu veikt vārdu filtrēšanas soli teksta priekšasptrādē, un pielietots uz apmācības datiem. 

\subsubsection{Vektorizācija}
Pirms algoritmu pielietošanas nepieciešams datus vektorizēt. Darba ietvaros uz katru no algoritmiem tika pārbaudītas dažādas vektorizācijas pieejas – vārdu maiss, bigrammu maiss, TF-IDF un vārdlietojuma kartējumi ar FastText, pielietojot katru no tām uz rāpuļa izgūtās datu kopas. Tika arī veikti eksperimenti ar word2vec vārdlietojuma kartējuma pieeju, tomēr Fasttext pielietošana sniedza labākus sākotnējos rezultātus, arī citi pētījumi par vārdlietojuma kartējuma pieejām latviešu valodas tekstiem \cite{LaucisJekabsonWordEmbedding} norāda FastText kā pieeju ar labāko veiktspēju, salīdzinot ar word2vec, ngram2vec, SSG (Structured Skip-Gram).

\subsection{Algoritmu implementācijas}

\subsubsection{Datu līdzsvarošana un sadale}
Svarīģs priekšnosacījums labai klasifikācijas modeļa apmācībai ir izvairīšanās no nevienmērīga klašu sadalījuma datu kopā. Ar rāpuli tika mēģināts izgūt samērā līdzīgu rakstu skaitu pa kategorijām, tomēr atšķirības eksistē, piemēram, klasei “Mūzika” rezultātā tika izgūti 1722 raksti, bet auto ziņām – 1232 raksti. Pirms apmācības visām kategorijām saglabājam 1200 rakstus, pārējos atmetot. Rezultātā iegūstam 12 000 rakstus, kurus tālāk sadalām – 80\% apmācībai (9600 raksti), 20\% validācijai (2400 raksti).

\subsubsection{Atbalsta vektora mašīnas}
Atbalsta vektora mašīnas apmācības algoritms ir implementēts ar scikit-learn komponenti LinearSVC (sklearn.svm.LinearSVC) ar dažādām vektorizācijas metodēm. Novērtēto akurātuma mēru ar dažādām vekorizācijas pieejām varam apskatīt \ref{tab:accuracy_svm} tabulā.
\begin{table}[H]
\centering
\caption{\label{tab:accuracy_svm}}
\textbf{Akurātuma mēri pielietojot AVM\\}
\begin{tabular}{|l|l|l|l|}
\hline
Vārdu maiss & Bigrammu maiss & TF-IDF & Fasttext \\ \hline
0.9696 & 0.9679 & 0.9738 & 0.9567 \\ \hline
\end{tabular}
\end{table}

\subsubsection{Naivā Bajesa metode}
Naivā Bajesa apmācības algoritms ir implementēts ar scikit-learn komponenti MultinomialNB (sklearn.naive\_bayes.MultinomialNB) ar dažādām vektorizācijas metodēm. Novērtēto akurātuma mēru ar dažādām vekorizācijas pieejām varam apskatīt \ref{tab:accuracy_nb} tabulā.
\begin{table}[H]
\centering
\caption{\label{tab:accuracy_nb}}
\textbf{Akurātuma mēri pielietojot Naivā Bajesa metodi\\}
\begin{tabular}{|l|l|l|l|}
\hline
Vārdu maiss & Bigrammu maiss & TF-IDF & Fasttext \\ \hline
0.9629 & 0.9558 & 0.9617 & 0.85 \\ \hline
\end{tabular}
\end{table}

\subsubsection{Loģistiskā regresija}
Loģistiskās regresijas apmācības algoritms ir implementēts ar scikit-learn komponenti LogisticRegression (sklearn.linear\_model.LogisticRegression) ar dažādām vektorizācijas metodēm. Novērtēto akurātuma mēru ar dažādām vekorizācijas pieejām varam apskatīt \ref{tab:accuracy_lr} tabulā.

\begin{table}[H]
\centering
\caption{\label{tab:accuracy_lr}}
\textbf{Akurātuma mēri pielietojot loģistisko regresiju\\}
\begin{tabular}{|l|l|l|l|}
\hline
Vārdu maiss & Bigrammu maiss & TF-IDF & Fasttext \\ \hline
0.9663 & 0.9663 & 0.9663 & 0.9579 \\ \hline
\end{tabular}
\end{table}

\subsubsection{Lēmumu koki}
Lēmumu koku pmācības algoritms ir implementēts ar scikit-learn komponenti DecisionTreeClassifier (sklearn.tree.DecisionTreeClassifier) ar dažādām vektorizācijas metodēm. Novērtēto akurātuma mēru ar dažādām vekorizācijas pieejām varam apskatīt \ref{tab:accuracy_dt}tabulā.

\begin{table}[H]
\centering
\caption{\label{tab:accuracy_dt}}
\textbf{Akurātuma mēri pielietojot lēmumu kokus\\}
\begin{tabular}{|l|l|l|l|}
\hline
Vārdu maiss & Bigrammu maiss & TF-IDF & Fasttext \\ \hline
0.8096 & 0.8196 & 0.7975 & 0.7408 \\ \hline
\end{tabular}
\end{table}


Vislabākais sasniegtais akurātums iegūts ar atbalsta vektora mašīnām un TF-IDF vektorizāciju - \textbf{0.9738}, šai pieejai varam vizualizēt kategorizāciju ar pārpratuma matricu. Kā redzams attēlā \ref{fig:AVM_TFIDF} – desmit  kategoriju klasifikācija tiek veikta ļoti precīzi un biežākā kļūda ir nepareizi klasificētas finanšu ziņas, klasificējot tās kā tehnoloģiju ziņas.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{AVM_TFIDF}
	\caption{Atbalsta vektora mašīnas pārpratuma matrica}
	\label{fig:AVM_TFIDF}
\end{figure}

\section{Neironu tīklu implementācija}
\subsection{Konvolūcijas neironu tīkli}
Konvolūcijas neironu tīklu implementēšanai izvēlēta Tensoflow bibliotēka ar Keras interfeisu, lai vieglāk varētu prototipēt un izstrādāt vēlamo neironu tīkla arhitektūru. Darba ietvarā apskatīta diezgan tradicionāla pieeja konvolūcijas neironu tīkliem teksta klasifikācijai, respektīvi, tiek definēti 5 slāņi:

\begin{itemize}
\item Iegulšanas slānis
\item Konvolūcijas slānis ar 1 dimensiju, izmantojot ReLU aktivizācijas funkciju
\item Apvienošanas slānis ar globālā maksimuma apvienošanu
\item Atmešanas slānis ar vērtību 0.5
\item Pilnīgi savienotais slānis gala klasifikācijai, izmantojot softmax aktivizācijas funkciju
\end{itemize}

Apmācība tika veikta 10 epohus, saglabājot modeli posmos ar mazāko validācijas zuduma (validation loss) vērtību, mēģinot izvairīties no pārmērīgas pielāgošanas.Akurātuma un zuduma evolūciju pa epohiem iespējams apskatīt \ref{fig:Accuracy_Loss_CNN_10epoch} attēlā.

Pielietojot augstāk minēto konvolūcijas neironu tīkla uzbūvi iegūstam akurātuma mēru kā 0.9571, kas lai gan ir diezgan labs rezultāts, tomēr ir mazāk precīzs par klasisku algoritmu (kā atbalsta vektora mašīnas) modeļiem. Pārpratuma matricu iespējams apskatīt attēlā \ref{fig:CNN}, kur varam novērot, ka arī ar šo pieeju visgrūtāk modelim ir atķirt tieši tehnoloģiju un finanšu ziņas.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{CNN}
	\caption{Konvolūcijas neironu tīkli - pārpratuma matrica}
	\label{fig:CNN}
\end{figure}

\begin{figure}[H]
	\includegraphics[width=\textwidth]{Accuracy_Loss_CNN_10epoch}
	\caption{Konvolūcijas neironu tīkli - novērtējums pa apmācības posmiem}
	\label{fig:Accuracy_Loss_CNN_10epoch}
\end{figure}

Meklējot iespējas uzlabot rezultātu ar konvolūcijas tīkliem tika izmēģināta arī Kim Yoon piedāvātā arhitektūra \cite{kimYoonCNN}, uzlabojot sākotnējo modeli ar 3 paralēliem konvolūcijas/apvienošanas slāņiem, kur katram konvolūcijas slānim pielietojam dažādus filtra izmērus.

\subsection{BERT}
Latviešu valodā ir pieejams BERT modelis ko izstrādājuši LU Matemātikas un informātikas institūta pētnieki Artūrs Znotiņš un Guntis Barzdiņš. Modelis ir apmācīts uz latviešu Wikipedia ierakstiem, tekstiem no valodas korpusa LVK2018, dažādiem ziņu rakstiem un to komentāriem, kas kopā veido 500 miljonu lielu tokenu kopu apmācībai. Ņemot par pamatu šo modeli un pielāgojot to klasifikācijas problēmai tika veikta apmācība, iegūstot modeli ar akurātuma mēru kā 0.9704.
Tā kā BERT noderīga daļa ir spēja uztvert teksta kontekstu, tekstu priekšapstrāde kā stopvārdu izņemšana netika pielietota, jo tādējādi varam pazaudēt daļu no konteksta un saiknēm starp vārdiem.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{Matrica_BERT}
	\caption{BERT - pārpratuma matrica}
	\label{fig:Matrica_BERT}
\end{figure}
